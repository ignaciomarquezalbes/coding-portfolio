# Conjugate Gradient Method – Numerical Solution of Linear Systems

This folder contains a MATLAB implementation of the Conjugate Gradient Method for solving symmetric positive definite (SPD) linear systems of equations.

**Repository link**: [https://github.com/ignaciomarquezalbes/coding-portfolio/tree/main/optimization-methods/conjugate-gradient-method](https://github.com/ignaciomarquezalbes/coding-portfolio/tree/main/optimization-methods/conjugate-gradient-method)

## Method Overview

The Conjugate Gradient Method is an iterative algorithm designed to solve numerically linear systems of the form:

A x = b,

where A is symmetric positive definite. This equivalent to minimizing the quadratic functional: 

f(x) = \frac{1}{2} x^T A x - b^T x,

Unlike the basic Gradient Method, the Conjugate Gradient method generates conjugate search directions that improve convergence, often requiring fewer iterations.

It solves the equivalent quadratic minimization problem:

\[
f(x) = \frac{1}{2} x^T A x - b^T x,
\]

by iteratively updating the solution using conjugate directions \( d_k \) instead of just the negative gradient. The main update formulas are:

\[
\begin{aligned}
r_k &= A x_k - b, \\
\alpha_k &= \frac{r_k^T r_k}{d_k^T A d_k}, \\
x_{k+1} &= x_k - \alpha_k d_k, \\
\beta_k &= \frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}, \\
d_{k+1} &= r_{k+1} + \beta_k d_k,
\end{aligned}
\]

where \( r_k \) is the residual at iteration \( k \), and the directions \( d_k \) are conjugate with respect to \( A \).

Iterations continue until the residual norm falls below a specified tolerance or a maximum number of iterations is reached.

## Folder Structure

- `main.m` — Entry point. Loads problem data, sets method parameters, and calls the solver.
- `grad_conj.m` — Implements the Conjugate Gradient Method. Logs residuals and other metrics at each iteration.
- `problem_data.m` — Contains problem parameters (matrix \( A \) and vector \( b \)).
- `gradconj.sal` — Output file generated by `grad_conj.m` containing iteration data and residual norms.

